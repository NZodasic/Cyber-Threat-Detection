{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlslnkKu+eKdEGjC66pij8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NZodasic/Cyber-Threat-Detection/blob/main/Super_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_all_models.py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Optional libs:\n",
        "# pip install pytorch-tabnet rtdl tabpfn\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import rtdl\n",
        "from tabpfn import TabPFNClassifier\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Load Data\n",
        "# -----------------------------\n",
        "CSV_PATH = \"pe_features_extended.csv\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# target\n",
        "y = (df['label'].str.lower() == 'malware').astype(int).values\n",
        "df = df.drop(columns=['label'])\n",
        "\n",
        "# numeric only (cho đơn giản, bạn có thể thêm text feature như trước)\n",
        "X = df.select_dtypes(include=[np.number]).fillna(0).values\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "results = []\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Define Models\n",
        "# -----------------------------\n",
        "\n",
        "# ---- MLP ----\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=[256,128]):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last_dim = in_dim\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(last_dim, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.3))\n",
        "            last_dim = h\n",
        "        layers.append(nn.Linear(last_dim, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "def train_mlp(X_train, X_test, y_train, y_test, epochs=10, batch=64):\n",
        "    model = MLP(X_train.shape[1]).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
        "                             torch.tensor(y_train, dtype=torch.float32))\n",
        "    test_ds  = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
        "                             torch.tensor(y_test, dtype=torch.float32))\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=256)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb,yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = criterion(model(xb), yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    model.eval()\n",
        "    preds, probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            out = torch.sigmoid(model(xb))\n",
        "            preds.extend((out.cpu().numpy()>=0.5).astype(int).ravel().tolist())\n",
        "            probs.extend(out.cpu().numpy().ravel().tolist())\n",
        "\n",
        "    return preds, probs\n",
        "\n",
        "# ---- ResNet for tabular ----\n",
        "class ResNetTabular(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=256, depth=4):\n",
        "        super().__init__()\n",
        "        self.fc_in = nn.Linear(in_dim, hidden)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden, hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.Linear(hidden, hidden)\n",
        "            ) for _ in range(depth)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc_in(x)\n",
        "        for block in self.blocks:\n",
        "            residual = x\n",
        "            x = block(x) + residual\n",
        "            x = torch.relu(x)\n",
        "        return self.fc_out(x).squeeze(1)\n",
        "\n",
        "def train_resnet(X_train, X_test, y_train, y_test, epochs=10, batch=64):\n",
        "    model = ResNetTabular(X_train.shape[1]).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
        "                             torch.tensor(y_train, dtype=torch.float32))\n",
        "    test_ds  = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
        "                             torch.tensor(y_test, dtype=torch.float32))\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=256)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb,yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = criterion(model(xb), yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    model.eval()\n",
        "    preds, probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            out = torch.sigmoid(model(xb))\n",
        "            preds.extend((out.cpu().numpy()>=0.5).astype(int).ravel().tolist())\n",
        "            probs.extend(out.cpu().numpy().ravel().tolist())\n",
        "\n",
        "    return preds, probs\n",
        "\n",
        "# ---- FT-Transformer ----\n",
        "def train_ft_transformer(X_train, X_test, y_train, y_test, epochs=10, batch=64):\n",
        "    d_numerical = X_train.shape[1]\n",
        "    model = rtdl.FTTransformer.make_baseline(\n",
        "        d_numerical=d_numerical,\n",
        "        categories=[],\n",
        "        d_out=1  # binary classification\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
        "                             torch.tensor(y_train, dtype=torch.float32))\n",
        "    test_ds  = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
        "                             torch.tensor(y_test, dtype=torch.float32))\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=256)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb,yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = criterion(model(xb), yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    model.eval()\n",
        "    preds, probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            out = torch.sigmoid(model(xb))\n",
        "            preds.extend((out.cpu().numpy()>=0.5).astype(int).ravel().tolist())\n",
        "            probs.extend(out.cpu().numpy().ravel().tolist())\n",
        "\n",
        "    return preds, probs\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Train & Evaluate\n",
        "# -----------------------------\n",
        "def evaluate_model(name, preds, probs, y_test):\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    prec = precision_score(y_test, preds)\n",
        "    rec = recall_score(y_test, preds)\n",
        "    f1 = f1_score(y_test, preds)\n",
        "    auc = roc_auc_score(y_test, probs)\n",
        "    results.append([name, acc, prec, rec, f1, auc])\n",
        "\n",
        "print(\"Training MLP...\")\n",
        "mlp_preds, mlp_probs = train_mlp(X_train, X_test, y_train, y_test)\n",
        "evaluate_model(\"MLP\", mlp_preds, mlp_probs, y_test)\n",
        "\n",
        "print(\"Training ResNet...\")\n",
        "res_preds, res_probs = train_resnet(X_train, X_test, y_train, y_test)\n",
        "evaluate_model(\"ResNet\", res_preds, res_probs, y_test)\n",
        "\n",
        "print(\"Training TabNet...\")\n",
        "tabnet = TabNetClassifier(verbose=0, seed=42)\n",
        "tabnet.fit(X_train, y_train, eval_set=[(X_test,y_test)], eval_metric=['auc'])\n",
        "tab_preds = tabnet.predict(X_test)\n",
        "tab_probs = tabnet.predict_proba(X_test)[:,1]\n",
        "evaluate_model(\"TabNet\", tab_preds, tab_probs, y_test)\n",
        "\n",
        "print(\"Training FT-Transformer...\")\n",
        "ft_preds, ft_probs = train_ft_transformer(X_train, X_test, y_train, y_test)\n",
        "evaluate_model(\"FT-Transformer\", ft_preds, ft_probs, y_test)\n",
        "\n",
        "print(\"Training TabPFN...\")\n",
        "tabpfn = TabPFNClassifier(N_ensemble_configurations=32)\n",
        "tabpfn.fit(X_train, y_train)\n",
        "tp_preds = tabpfn.predict(X_test)\n",
        "tp_probs = tabpfn.predict_proba(X_test)[:,1]\n",
        "evaluate_model(\"TabPFN\", tp_preds, tp_probs, y_test)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Kết quả cuối cùng\n",
        "# -----------------------------\n",
        "df_results = pd.DataFrame(results, columns=[\"Model\",\"Accuracy\",\"Precision\",\"Recall\",\"F1\",\"ROC AUC\"])\n",
        "print(\"\\n=== Final Comparison ===\")\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "id": "AW-ruVROkvwF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}