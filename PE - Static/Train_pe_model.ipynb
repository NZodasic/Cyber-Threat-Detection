{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYV1HsZQD6M6CvI+QGYC+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NZodasic/Cyber-Threat-Detection/blob/main/Train_pe_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "from scipy import sparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler"
      ],
      "metadata": {
        "id": "xHWF_qeOcHcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "LU1RzG15dwmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load data\n",
        "CSV_PATH = \"pe_features_extended.csv\"   # đổi theo đường dẫn của bạn\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "print(\"Samples per class:\\n\", df['label'].value_counts())"
      ],
      "metadata": {
        "id": "9XiaRtOJcJGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preprocess / Feature handling\n",
        "# create imports_text from import_0..import_9 if present\n",
        "import_cols = [c for c in df.columns if c.startswith(\"import_\")]\n",
        "if import_cols:\n",
        "    df['imports_text'] = df[import_cols].fillna(\"\").astype(str).apply(\n",
        "        lambda row: \" \".join([t for t in row if t and t != 'nan']), axis=1\n",
        "    )"
      ],
      "metadata": {
        "id": "iPllcXTAd0qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop obviously useless columns\n",
        "drop_like = [c for c in df.columns if c.lower() in ('md5','filename','filepath','file_path')]\n",
        "df = df.drop(columns=[c for c in drop_like if c in df.columns])\n",
        "\n",
        "# Drop object columns except 'imports_text' and 'label'\n",
        "obj_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "for c in obj_cols:\n",
        "    if c not in ('imports_text', 'label'):\n",
        "        df = df.drop(columns=[c])\n",
        "\n",
        "# Map label to binary\n",
        "y = (df['label'].str.lower() == 'malware').astype(int).values\n",
        "df = df.drop(columns=['label'])\n",
        "\n",
        "# Numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# If imports_text does not exist, create empty\n",
        "if 'imports_text' not in df.columns:\n",
        "    df['imports_text'] = \"\"\n",
        "\n",
        "# ColumnTransformer: numeric pipeline + text vectorizer\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "MAX_IMPORT_FEATURES = 500   # tune this\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', num_pipeline, numeric_cols),\n",
        "    ('imp', CountVectorizer(max_features=MAX_IMPORT_FEATURES, token_pattern=r\"(?u)\\b\\w+\\b\"), 'imports_text')\n",
        "], remainder='drop', sparse_threshold=0.3)"
      ],
      "metadata": {
        "id": "9gNduzBTcLS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 3. Train/test split (stratified)\n",
        "# -------------------------\n",
        "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
        "    df, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Fit preprocessor on train\n",
        "preprocessor.fit(X_train_df)\n",
        "\n",
        "# transform to arrays (may be sparse)\n",
        "X_train_trans = preprocessor.transform(X_train_df)\n",
        "X_test_trans  = preprocessor.transform(X_test_df)\n",
        "\n",
        "# Convert to dense if small enough, else keep sparse (RandomForest requires dense)\n",
        "# Here we convert to dense — if you have memory issues, reduce MAX_IMPORT_FEATURES\n",
        "if sparse.issparse(X_train_trans):\n",
        "    X_train = X_train_trans.toarray()\n",
        "    X_test  = X_test_trans.toarray()\n",
        "else:\n",
        "    X_train = X_train_trans\n",
        "    X_test  = X_test_trans\n",
        "\n",
        "print(\"Feature matrix shape:\", X_train.shape)"
      ],
      "metadata": {
        "id": "xPIKONspdpVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 4. Baseline model: RandomForest with class_weight='balanced'\n",
        "# -------------------------\n",
        "rf = RandomForestClassifier(n_estimators=200, n_jobs=-1, class_weight='balanced', random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "y_prob = rf.predict_proba(X_test)[:,1]\n",
        "\n",
        "cm_rf = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Benign\",\"Malware\"], yticklabels=[\"Benign\",\"Malware\"])\n",
        "plt.title(\"RandomForest Confusion Matrix\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"=== RandomForest Report ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Save rf model + preprocessor\n",
        "joblib.dump(rf, \"rf_pe_model.joblib\")\n",
        "joblib.dump(preprocessor, \"preprocessor.joblib\")\n",
        "print(\"[+] Saved RandomForest model and preprocessor.\")"
      ],
      "metadata": {
        "id": "q7rEeSaSdqSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AwWkbdLbwA9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Prepare tensors\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Sample weights to handle imbalance (WeightedRandomSampler)\n",
        "class_sample_count = np.bincount(y_train)\n",
        "weights = 1.0 / class_sample_count\n",
        "samples_weight = weights[y_train]\n",
        "samples_weight = torch.from_numpy(samples_weight.astype(np.float32))\n",
        "sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n",
        "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=256, shuffle=False)\n",
        "\n",
        "# Simple MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "model = MLP(X_train.shape[1]).to(device)\n",
        "\n",
        "# use pos_weight to penalize false negatives\n",
        "pos = max(1, int(y_train.sum()))\n",
        "neg = len(y_train) - pos\n",
        "pos_weight = torch.tensor([neg / pos], dtype=torch.float32).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train loop\n",
        "EPOCHS = 10\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # eval on test\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    probs = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            out = model(xb)\n",
        "            prob = torch.sigmoid(out).cpu().numpy()\n",
        "            pred = (prob >= 0.5).astype(int).ravel()\n",
        "            probs.extend(prob.ravel().tolist())\n",
        "            preds.extend(pred.tolist())\n",
        "    roc = roc_auc_score(y_test, probs)\n",
        "    history[\"loss\"].append(avg_loss)\n",
        "    history[\"roc\"].append(roc)\n",
        "    print(f\"Epoch {epoch:02d} | loss={avg_loss:.4f} | test ROC={roc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5c. Vẽ learning curves\n",
        "# -------------------------\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(1,EPOCHS+1), history[\"loss\"], marker='o')\n",
        "plt.title(\"MLP Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(1,EPOCHS+1), history[\"roc\"], marker='o', color=\"green\")\n",
        "plt.title(\"MLP Test ROC AUC\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"ROC AUC\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kXncFOMdfKin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 5d. Confusion Matrix cho MLP\n",
        "# -------------------------\n",
        "cm_mlp = confusion_matrix(y_test, preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_mlp, annot=True, fmt=\"d\", cmap=\"Oranges\", xticklabels=[\"Benign\",\"Malware\"], yticklabels=[\"Benign\",\"Malware\"])\n",
        "plt.title(\"MLP Confusion Matrix\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C7DHpnvRfL9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation\n",
        "print(\"=== PyTorch MLP Report ===\")\n",
        "print(classification_report(y_test, preds, digits=4))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, probs))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, preds))"
      ],
      "metadata": {
        "id": "Kvw9kqnSdr9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and preprocessor\n",
        "torch.save(model.state_dict(), \"pe_mlp.pt\")\n",
        "joblib.dump(preprocessor, \"preprocessor.joblib\")  # already saved but safe\n",
        "print(\"[+] Saved PyTorch model and preprocessor.\")"
      ],
      "metadata": {
        "id": "kv0unX2CdstW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}