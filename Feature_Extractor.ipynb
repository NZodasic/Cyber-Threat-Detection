{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkOQ5Hj1hVx0MUtAEwf6Fi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pefile\n",
        "import math\n",
        "import csv\n",
        "import hashlib\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Fz22xRaaeHEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Utility ======\n",
        "def get_entropy(data):\n",
        "    if not data:\n",
        "        return 0.0\n",
        "    counter = Counter(data)\n",
        "    length = len(data)\n",
        "    entropy = 0\n",
        "    for count in counter.values():\n",
        "        p_x = count / length\n",
        "        entropy -= p_x * math.log2(p_x)\n",
        "    return entropy\n",
        "\n",
        "def get_md5(file_path):\n",
        "    h = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        while chunk := f.read(8192):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()"
      ],
      "metadata": {
        "id": "q-o-7nbaeIS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Feature Extractor ======\n",
        "def extract_features(file_path, label):\n",
        "    try:\n",
        "        pe = pefile.PE(file_path)\n",
        "        features = {}\n",
        "\n",
        "        # --- File-level ---\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            data = f.read()\n",
        "            features[\"FileSize\"] = len(data)\n",
        "            features[\"FileEntropy\"] = round(get_entropy(data), 3)\n",
        "        features[\"MD5\"] = get_md5(file_path)\n",
        "\n",
        "        # --- Header ---\n",
        "        features[\"TimeDateStamp\"] = pe.FILE_HEADER.TimeDateStamp\n",
        "        features[\"Machine\"] = pe.FILE_HEADER.Machine\n",
        "        features[\"Characteristics\"] = pe.FILE_HEADER.Characteristics\n",
        "        features[\"NumberOfSections\"] = pe.FILE_HEADER.NumberOfSections\n",
        "        features[\"AddressOfEntryPoint\"] = pe.OPTIONAL_HEADER.AddressOfEntryPoint\n",
        "        features[\"SizeOfImage\"] = pe.OPTIONAL_HEADER.SizeOfImage\n",
        "        features[\"SizeOfHeaders\"] = pe.OPTIONAL_HEADER.SizeOfHeaders\n",
        "        features[\"Subsystem\"] = pe.OPTIONAL_HEADER.Subsystem\n",
        "        features[\"DllCharacteristics\"] = pe.OPTIONAL_HEADER.DllCharacteristics\n",
        "        features[\"LinkerVersion\"] = float(f\"{pe.OPTIONAL_HEADER.MajorLinkerVersion}.{pe.OPTIONAL_HEADER.MinorLinkerVersion}\")\n",
        "        features[\"Checksum\"] = pe.OPTIONAL_HEADER.CheckSum\n",
        "\n",
        "        # --- TLS ---\n",
        "        features[\"TLS_Callbacks\"] = hasattr(pe, \"DIRECTORY_ENTRY_TLS\")\n",
        "\n",
        "        # --- Sections ---\n",
        "        section_entropy = []\n",
        "        section_ratio = []\n",
        "        for section in pe.sections:\n",
        "            vsize = section.Misc_VirtualSize\n",
        "            raw_size = section.SizeOfRawData\n",
        "            entropy = get_entropy(section.get_data())\n",
        "            section_entropy.append(entropy)\n",
        "            if raw_size != 0:\n",
        "                section_ratio.append(vsize / raw_size)\n",
        "        features[\"MeanSectionEntropy\"] = round(sum(section_entropy) / len(section_entropy), 3) if section_entropy else 0\n",
        "        features[\"MeanSectionRatio\"] = round(sum(section_ratio) / len(section_ratio), 3) if section_ratio else 0\n",
        "\n",
        "        # --- Imports ---\n",
        "        imports_count = 0\n",
        "        suspicious_apis = [\"VirtualAlloc\", \"WriteProcessMemory\", \"CreateRemoteThread\", \"WinExec\", \"ShellExecute\"]\n",
        "        suspicious_found = 0\n",
        "        if hasattr(pe, \"DIRECTORY_ENTRY_IMPORT\"):\n",
        "            for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
        "                for imp in entry.imports:\n",
        "                    imports_count += 1\n",
        "                    if imp.name:\n",
        "                        if any(api.lower() in imp.name.decode(errors=\"ignore\").lower() for api in suspicious_apis):\n",
        "                            suspicious_found += 1\n",
        "        features[\"ImportCount\"] = imports_count\n",
        "        features[\"SuspiciousAPIs\"] = suspicious_found\n",
        "\n",
        "        # --- Exports ---\n",
        "        exports_count = 0\n",
        "        if hasattr(pe, \"DIRECTORY_ENTRY_EXPORT\"):\n",
        "            exports_count = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)\n",
        "        features[\"ExportCount\"] = exports_count\n",
        "\n",
        "        # --- Resources ---\n",
        "        res_count = 0\n",
        "        res_size = 0\n",
        "        if hasattr(pe, \"DIRECTORY_ENTRY_RESOURCE\"):\n",
        "            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:\n",
        "                if hasattr(resource_type, \"directory\"):\n",
        "                    for entry in resource_type.directory.entries:\n",
        "                        if hasattr(entry, \"directory\"):\n",
        "                            for subentry in entry.directory.entries:\n",
        "                                data_rva = subentry.data.struct.OffsetToData\n",
        "                                size = subentry.data.struct.Size\n",
        "                                res_size += size\n",
        "                                res_count += 1\n",
        "        features[\"ResourceCount\"] = res_count\n",
        "        features[\"ResourceSize\"] = res_size\n",
        "\n",
        "        # --- Strings (basic count only) ---\n",
        "        try:\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                data = f.read()\n",
        "                strings = [s.decode(errors=\"ignore\") for s in data.split(b\"\\x00\") if len(s) > 4]\n",
        "                features[\"StringsCount\"] = len(strings)\n",
        "        except:\n",
        "            features[\"StringsCount\"] = 0\n",
        "\n",
        "        # --- Label ---\n",
        "        features[\"Label\"] = label\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[!] Error parsing {file_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "vpRI3dkpeKBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Dataset Scanner ======\n",
        "def scan_dataset(root_dir, output_csv):\n",
        "    rows = []\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith((\".exe\", \".dll\")):\n",
        "                file_path = os.path.join(root, file)\n",
        "\n",
        "                # Label: dựa trên tên thư mục\n",
        "                label = \"Benign\" if \"Benign\" in root else \"Malware\"\n",
        "\n",
        "                feats = extract_features(file_path, label)\n",
        "                if feats:\n",
        "                    rows.append(feats)\n",
        "\n",
        "    # Save to CSV\n",
        "    if rows:\n",
        "        with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(rows)\n",
        "    print(f\"[+] Done! Extracted {len(rows)} samples -> {output_csv}\")"
      ],
      "metadata": {
        "id": "ohf5KzQNeOc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjUBJJC9eFaM"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"Dataset\"   # thư mục gốc Dataset\n",
        "output_csv = \"pe_features_dataset.csv\"\n",
        "scan_dataset(dataset_dir, output_csv)\n"
      ]
    }
  ]
}