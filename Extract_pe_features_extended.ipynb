{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4RbhPgnBtHy5tuaTGUFSD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pefile\n",
        "import math\n",
        "import csv\n",
        "import hashlib\n",
        "from collections import Counter, defaultdict"
      ],
      "metadata": {
        "id": "M1TLd6fTZxZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KNOWN_SECTIONS = [\n",
        "    \".text\", \".rdata\", \".data\", \".idata\", \".edata\",\n",
        "    \".pdata\", \".rsrc\", \".reloc\", \".bss\", \".tls\", \".debug\"\n",
        "]\n",
        "\n",
        "# normalize dll names lower-case for matching\n",
        "KNOWN_DLLS = [\n",
        "    \"kernel32.dll\", \"advapi32.dll\", \"user32.dll\", \"gdi32.dll\",\n",
        "    \"ntdll.dll\", \"ws2_32.dll\", \"wsock32.dll\", \"wininet.dll\"\n",
        "]\n",
        "\n",
        "SUSPICIOUS_APIS = [\n",
        "    \"VirtualAlloc\", \"VirtualAllocEx\", \"VirtualProtect\",\n",
        "    \"WriteProcessMemory\", \"CreateRemoteThread\", \"LoadLibrary\",\n",
        "    \"GetProcAddress\", \"WinExec\", \"ShellExecute\", \"URLDownloadToFile\"\n",
        "]"
      ],
      "metadata": {
        "id": "c_D8asxYZyPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entropy(data):\n",
        "    if not data:\n",
        "        return 0.0\n",
        "    counter = Counter(data)\n",
        "    length = len(data)\n",
        "    entropy = 0.0\n",
        "    for count in counter.values():\n",
        "        p_x = count / length\n",
        "        entropy -= p_x * math.log2(p_x)\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "cMcfWEqpZ2g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def md5_file(path):\n",
        "    import hashlib\n",
        "    h = hashlib.md5()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()"
      ],
      "metadata": {
        "id": "ESm8SAqzZ3td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_decode(b):\n",
        "    try:\n",
        "        return b.decode(errors=\"ignore\")\n",
        "    except:\n",
        "        return str(b)"
      ],
      "metadata": {
        "id": "H1wLxX8zZ4-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(file_path, label):\n",
        "    feats = {}\n",
        "    try:\n",
        "        pe = pefile.PE(file_path, fast_load=True)\n",
        "        pe.parse_data_directories(directories=[\n",
        "            pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT'],\n",
        "            pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXPORT'],\n",
        "            pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_RESOURCE'],\n",
        "            pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_TLS']\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        print(f\"[!] Cannot parse PE {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # File-level\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    feats[\"MD5\"] = md5_file(file_path)\n",
        "    feats[\"FileSize\"] = len(data)\n",
        "    feats[\"FileEntropy\"] = round(get_entropy(data), 3)\n",
        "\n",
        "    # Header\n",
        "    try:\n",
        "        fh = pe.FILE_HEADER\n",
        "        oh = pe.OPTIONAL_HEADER\n",
        "        feats[\"TimeDateStamp\"] = fh.TimeDateStamp\n",
        "        feats[\"Machine\"] = fh.Machine\n",
        "        feats[\"Characteristics\"] = fh.Characteristics\n",
        "        feats[\"NumberOfSections\"] = fh.NumberOfSections\n",
        "        feats[\"AddressOfEntryPoint\"] = oh.AddressOfEntryPoint\n",
        "        feats[\"SizeOfImage\"] = oh.SizeOfImage\n",
        "        feats[\"SizeOfHeaders\"] = oh.SizeOfHeaders\n",
        "        feats[\"Subsystem\"] = oh.Subsystem\n",
        "        feats[\"DllCharacteristics\"] = oh.DllCharacteristics\n",
        "        feats[\"LinkerVersion\"] = f\"{oh.MajorLinkerVersion}.{oh.MinorLinkerVersion}\"\n",
        "        feats[\"Checksum\"] = oh.CheckSum\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # TLS callbacks flag\n",
        "    feats[\"Has_TLS\"] = 1 if hasattr(pe, \"DIRECTORY_ENTRY_TLS\") else 0\n",
        "\n",
        "    # Section-level: presence flags + per-section entropy/size/ratio (if present)\n",
        "    section_map = {sec.Name.decode(errors=\"ignore\").rstrip(\"\\x00\"): sec for sec in pe.sections}\n",
        "    # Normalize names (lowercase, ensure leading dot)\n",
        "    normalized_names = {name.lower() if name.startswith('.') else ('.'+name.lower()): name for name in section_map.keys()}\n",
        "\n",
        "    # initialize section presence and stats\n",
        "    for s in KNOWN_SECTIONS:\n",
        "        key_presence = f\"sec_present_{s.strip('.')}\"\n",
        "        feats[key_presence] = 0\n",
        "        feats[f\"sec_entropy_{s.strip('.')}\"] = 0.0\n",
        "        feats[f\"sec_virtualsize_{s.strip('.')}\"] = 0\n",
        "        feats[f\"sec_rawsize_{s.strip('.')}\"] = 0\n",
        "        feats[f\"sec_ratio_{s.strip('.')}\"] = 0.0\n",
        "\n",
        "    # fill actual\n",
        "    for raw_name, sec in section_map.items():\n",
        "        name = raw_name.decode(errors=\"ignore\").rstrip(\"\\x00\") if isinstance(raw_name, bytes) else raw_name\n",
        "        lname = name.lower()\n",
        "        # try to match known section if possible\n",
        "        matched = None\n",
        "        for s in KNOWN_SECTIONS:\n",
        "            if lname == s or lname.startswith(s):\n",
        "                matched = s\n",
        "                break\n",
        "        if matched is None:\n",
        "            # not in known list; create generic features for unknown sections count\n",
        "            feats.setdefault(\"unknown_section_count\", 0)\n",
        "            feats[\"unknown_section_count\"] += 1\n",
        "            # you may optionally store unknown section names\n",
        "        else:\n",
        "            feats[f\"sec_present_{matched.strip('.')}\"] = 1\n",
        "            raw = sec.get_data()\n",
        "            ent = round(get_entropy(raw), 3)\n",
        "            feats[f\"sec_entropy_{matched.strip('.')}\"] = ent\n",
        "            feats[f\"sec_virtualsize_{matched.strip('.')}\"] = sec.Misc_VirtualSize\n",
        "            feats[f\"sec_rawsize_{matched.strip('.')}\"] = sec.SizeOfRawData\n",
        "            try:\n",
        "                feats[f\"sec_ratio_{matched.strip('.')}\"] = round(sec.Misc_VirtualSize / sec.SizeOfRawData, 3) if sec.SizeOfRawData else 0.0\n",
        "            except Exception:\n",
        "                feats[f\"sec_ratio_{matched.strip('.')}\"] = 0.0\n",
        "\n",
        "    # Section-level summary\n",
        "    entropies = []\n",
        "    ratios = []\n",
        "    for sec in pe.sections:\n",
        "        try:\n",
        "            entropies.append(get_entropy(sec.get_data()))\n",
        "            if sec.SizeOfRawData:\n",
        "                ratios.append(sec.Misc_VirtualSize / sec.SizeOfRawData)\n",
        "        except:\n",
        "            pass\n",
        "    feats[\"mean_section_entropy\"] = round(sum(entropies)/len(entropies), 3) if entropies else 0.0\n",
        "    feats[\"mean_section_ratio\"] = round(sum(ratios)/len(ratios), 3) if ratios else 0.0\n",
        "\n",
        "    # Imports: count, dll presence flags, suspicious api count, top imports\n",
        "    import_count = 0\n",
        "    dll_presence = {dll:0 for dll in KNOWN_DLLS}\n",
        "    suspicious_api_count = 0\n",
        "    top_imports = []  # collect names\n",
        "    try:\n",
        "        if hasattr(pe, \"DIRECTORY_ENTRY_IMPORT\"):\n",
        "            for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
        "                dllname = safe_decode(entry.dll).lower()\n",
        "                import_count += len(entry.imports)\n",
        "                # mark known DLLs presence\n",
        "                for known in KNOWN_DLLS:\n",
        "                    if known in dllname:\n",
        "                        dll_presence[known] = 1\n",
        "                # functions\n",
        "                for imp in entry.imports:\n",
        "                    if imp.name:\n",
        "                        iname = imp.name.decode(errors=\"ignore\")\n",
        "                        top_imports.append(iname)\n",
        "                        for api in SUSPICIOUS_APIS:\n",
        "                            if api.lower() in iname.lower():\n",
        "                                suspicious_api_count += 1\n",
        "                    else:\n",
        "                        top_imports.append(str(imp.ordinal))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    feats[\"import_count\"] = import_count\n",
        "    feats[\"suspicious_api_count\"] = suspicious_api_count\n",
        "    # add dll presence flags\n",
        "    for dll, val in dll_presence.items():\n",
        "        feats[f\"dll_{dll.replace('.', '_')}\"] = val\n",
        "\n",
        "    # store first N imports as separate features (optional)\n",
        "    N = 10\n",
        "    for i in range(N):\n",
        "        feats[f\"import_{i}\"] = top_imports[i] if i < len(top_imports) else \"\"\n",
        "\n",
        "    # Exports\n",
        "    export_count = 0\n",
        "    try:\n",
        "        if hasattr(pe, \"DIRECTORY_ENTRY_EXPORT\") and pe.DIRECTORY_ENTRY_EXPORT:\n",
        "            export_count = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)\n",
        "    except:\n",
        "        pass\n",
        "    feats[\"export_count\"] = export_count\n",
        "\n",
        "    # Resources: count and total size\n",
        "    res_count = 0\n",
        "    res_total_size = 0\n",
        "    try:\n",
        "        if hasattr(pe, \"DIRECTORY_ENTRY_RESOURCE\"):\n",
        "            for res_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:\n",
        "                # navigate directories to resource data\n",
        "                def traverse(node):\n",
        "                    nonlocal res_count, res_total_size\n",
        "                    if hasattr(node, 'directory'):\n",
        "                        for e in node.directory.entries:\n",
        "                            traverse(e)\n",
        "                    elif hasattr(node, 'data'):\n",
        "                        try:\n",
        "                            res_count += 1\n",
        "                            res_total_size += node.data.struct.Size\n",
        "                        except:\n",
        "                            pass\n",
        "                traverse(res_type)\n",
        "    except Exception:\n",
        "        pass\n",
        "    feats[\"resource_count\"] = res_count\n",
        "    feats[\"resource_total_size\"] = res_total_size\n",
        "\n",
        "    # Strings: count of printable strings > length threshold, suspicious keywords\n",
        "    printable_strings = []\n",
        "    try:\n",
        "        parts = data.split(b'\\x00')\n",
        "        for p in parts:\n",
        "            if len(p) >= 4:\n",
        "                try:\n",
        "                    s = p.decode('utf-8', errors='ignore')\n",
        "                    if any(c.isalnum() for c in s):\n",
        "                        printable_strings.append(s)\n",
        "                except:\n",
        "                    pass\n",
        "    except:\n",
        "        pass\n",
        "    feats[\"strings_count\"] = len(printable_strings)\n",
        "\n",
        "    suspicious_kw = [\"http://\", \"https://\", \"hkey_local_machine\", \"cmd.exe\", \"powershell\", \".exe\", \".dll\"]\n",
        "    feats[\"suspicious_string_count\"] = sum(1 for s in printable_strings if any(k in s.lower() for k in suspicious_kw))\n",
        "\n",
        "    # Label\n",
        "    feats[\"label\"] = label\n",
        "\n",
        "    return feats"
      ],
      "metadata": {
        "id": "RdlhxSkGZ903"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ_Fe2xoZvG6"
      },
      "outputs": [],
      "source": [
        "def scan_and_save(dataset_root, out_csv):\n",
        "    rows = []\n",
        "    for root, _, files in os.walk(dataset_root):\n",
        "        for fn in files:\n",
        "            if fn.lower().endswith((\".exe\", \".dll\")):\n",
        "                path = os.path.join(root, fn)\n",
        "                # labeling logic: you can refine (multi-class) based on folder structure\n",
        "                label = \"Benign\" if \"benign\" in root.lower() else \"Malware\"\n",
        "                feats = extract_features(path, label)\n",
        "                if feats:\n",
        "                    rows.append(feats)\n",
        "\n",
        "    if not rows:\n",
        "        print(\"No samples processed.\")\n",
        "        return\n",
        "\n",
        "    # ensure deterministic field order\n",
        "    fieldnames = sorted(rows[0].keys())\n",
        "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
        "        writer = csv.DictWriter(fout, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            # ensure all fields exist\n",
        "            for f in fieldnames:\n",
        "                if f not in r:\n",
        "                    r[f] = \"\"\n",
        "            writer.writerow(r)\n",
        "    print(f\"[+] Saved {len(rows)} samples to {out_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    dataset_dir = \"Dataset\"   # change if needed\n",
        "    output_file = \"pe_features_extended.csv\"\n",
        "    scan_and_save(dataset_dir, output_file)"
      ],
      "metadata": {
        "id": "Bf5ec-hYaAtL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}